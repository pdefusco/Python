{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook putting together concepts from all modeling notebook to construct final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import KFold,cross_val_score, cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "from sklearn.feature_selection import RFE, f_regression, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "from airbnb_modeling import detect_feature_importance, scale_data, normalize_data, eval_metrics, plot_residuals, plot_predictions\n",
    "from parse_methods import parse_columns\n",
    "from airbnb_modeling import detect_interactions, add_interactions, map_variable\n",
    "#from filename import methodname\n",
    "#from airbnb_modeling import\n",
    "#from airbnb_modeling import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = pd.read_csv('Datasources/listings_augmented/listings_augmented_2018-05-31_V3.csv',low_memory=False)\n",
    "listings = listings.drop(listings.index[4323:4325])\n",
    "listings.index = [i for i in range(len(listings))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Excluded variables from the featuresExploration notebook\n",
    "%store -r excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [i for i in listings.columns if i not in excluded]\n",
    "X = listings[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bathrooms    10\n",
       "bedrooms      3\n",
       "beds          4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X.columns[X.isnull().any()]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = parse_columns(X, ['has_Pets_Allowed',\n",
    " 'has_Wheelchair_Accessible',\n",
    " 'has_First_Aid_Kit',\n",
    " 'has_Cat(s)',\n",
    " 'has_24-Hour_Check-in',\n",
    " 'uses_jumio',\n",
    "'description-Topic0',\n",
    " 'description-Topic1',\n",
    " 'description-Topic4',\n",
    " 'description-Topic5',\n",
    " 'description-Topic6',\n",
    " 'description-Topic10',\n",
    " 'description-Topic11',\n",
    " 'description-Topic12',\n",
    " 'description-Topic13',\n",
    " 'description-Topic15',\n",
    " 'description-Topic17',\n",
    " 'description-Topic18',\n",
    " 'description-Dominant_Topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scale_data(X)\n",
    "X_normed = normalize_data(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_scaled = X_scaled['price_y'].fillna(X_scaled['price_y'].mean())\n",
    "X_scaled = X_scaled[X_scaled.columns.drop(X_scaled[list(X_scaled.filter(regex='price'))])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_normed = X_normed['price_y'].fillna(X_normed['price_y'].mean())\n",
    "X_normed = X_normed[X_normed.columns.drop(X_normed[list(X_normed.filter(regex='price'))])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Simple Model with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Final Number of Features Used: ', len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_train = lin_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Training R2: ',r2_score(y_train, predictions_train)\n",
    "print 'Training RMSE: ',np.sqrt(mean_squared_error(y_train, predictions_train))\n",
    "print 'Training MAE: ',mean_absolute_error(y_train, predictions_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_reg = DecisionTreeRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions_train = tree_reg.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Training R2: ',r2_score(y_train, predictions_train)\n",
    "print 'Training RMSE: ',np.sqrt(mean_squared_error(y_train, predictions_train))\n",
    "print 'Training MAE: ',mean_absolute_error(y_train, predictions_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_lin = cross_validate(lin_reg, X_train, y_train, cv=10, \n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_tree = cross_validate(tree_reg, X_train, y_train, cv=10, \n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with CV: '\n",
    "eval_metrics(scores_lin)\n",
    "print '----'\n",
    "print '----'\n",
    "print 'Evaluation Metrics for Tree Regression with CV: '\n",
    "eval_metrics(scores_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plots the RMSE for train and validation as a function of the number of instances in the training set\n",
    "def plot_rmse_instances(clf, X_train, y_train):\n",
    "\n",
    "    train_errors, validation_errors = [],[]\n",
    "    \n",
    "    n = 2\n",
    "    \n",
    "    for i in range(n+1,len(X_train)):\n",
    "        \n",
    "        cv_results = cross_validate(clf,X_train[:i],y_train[:i],\n",
    "                                   scoring='neg_mean_squared_error',\n",
    "                                   cv=n)\n",
    "        train_score = np.sqrt(-cv_results['train_score'].mean())\n",
    "        val_score = np.sqrt(-cv_results['test_score'].mean())\n",
    "        \n",
    "        train_errors.append(train_score)\n",
    "        validation_errors.append(val_score)    \n",
    "    \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(validation_errors), \"b-\", linewidth=2, label='validation')\n",
    "    plt.xlabel('Number of Instances')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Train and Val RMSE\\'s as a Function of Number of Instances')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "plot_rmse_instances(lin_reg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Plot the RMSE for training and validation as a function of the number of features used\n",
    "#ranked features is a list of features sorted by importance - descending\n",
    "def plot_rmse_features(clf, X_train, y_train, ranked_features):\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_train[ranked_features], y_train, test_size=0.3)\n",
    "    \n",
    "    train_errors, validation_errors = [],[]\n",
    "    \n",
    "    for i in range(3,len(ranked_features)):\n",
    "        clf.fit(X_train.ix[:,2:i],y_train)\n",
    "        y_train_predict = clf.predict(X_train.ix[:,2:i])\n",
    "        y_val_predict = clf.predict(X_val.ix[:,2:i])\n",
    "        train_errors.append(mean_squared_error(y_train_predict, y_train))\n",
    "        validation_errors.append(mean_squared_error(y_val_predict, y_val))\n",
    "    \n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(validation_errors), \"b-\", linewidth=2, label='validation')\n",
    "    plt.xlabel('Number of Features')\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.title('Train and Val RMSE\\'s as a Function of Number of Features')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rmse_features(lin_reg, X_normed, y_normed,X_normed.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "selector = RFECV(estimator, step=1, cv=5, scoring='neg_mean_squared_error')\n",
    "selector.fit(X_normed, y_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation RMSE score\")\n",
    "plt.title(\"Optimal number of features : %d\" % selector.n_features_)\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), np.sqrt(-selector.grid_scores_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = selector.transform(X_normed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features have equally important ranking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try Linear Regression with CV again and let's see if we have any improvements with the transformed version of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_new,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg.fit(X_train,y_train)\n",
    "scores_lin = cross_validate(lin_reg, X_train, y_train, cv=10, \n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))\n",
    "print 'Evaluation Metrics for Linear Regression with CV: '\n",
    "eval_metrics(scores_lin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Validation Metrics have all (slightly) improved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Here we would first rebuild the model only with the important features i.e. number of features where val error is lowest. But in our case, all features are equally important so there is no need to do that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_normed.columns, selector.ranking_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'RFECV_Ranking'})\n",
    "importances = importances.sort_values(by='RFECV_Ranking')\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let's add interaction features to see if our model will improve. We only select the interaction features that yield an increase in R2 above a certain arbitrary threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)\n",
    "increments = detect_interactions(X_normed,y_normed, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_normed_wint = add_interactions(X_normed, increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "increments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_int, X_test_int, y_train, y_test = train_test_split(X_normed_wint,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg_int = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg_int.fit(X_train_int, y_train)\n",
    "scores_lin_int = cross_validate(lin_reg_int, X_train_int, y_train, cv=10, scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with CV and Interactions Features: '\n",
    "eval_metrics(scores_lin_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick only interaction features that minimize CV RMSE by feeding the new data into RCEFV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "selector = RFECV(estimator, step=1, cv=5, scoring='neg_mean_squared_error')\n",
    "selector.fit(X_train_int, y_train)\n",
    "print(\"Optimal number of features : %d\" % selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation RMSE score\")\n",
    "plt.title(\"Optimal number of features : %d\" % selector.n_features_)\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), np.sqrt(-selector.grid_scores_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it worth adding creating a new model with only important features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_new = selector.transform(X_normed)\n",
    "#Add rest if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train_int.columns, selector.ranking_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'RFECV_Ranking'})\n",
    "importances = importances.sort_values(by='RFECV_Ranking', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features = list(importances.head(selector.n_features_).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_rmse_features(lin_reg_int, X_train_int, y_train,best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how either of the two models performs against our Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions_int = lin_reg_int.predict(X_test_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for LR with no Interactions'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for LR with Interactions'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_int)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_int))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_int, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Plot for LR with no Interactions'\n",
    "plot_residuals(X_test,y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Plot for LR with Interactions'\n",
    "plot_residuals(X_test,y_test,test_predictions_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Plot for LR with no Interactions'\n",
    "plot_predictions(y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Plot for LR with Interactions'\n",
    "plot_predictions(y_test,test_predictions_int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment about results with interactions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce some degree of regularization to see if we can decrease validation RMSE even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "param_grid = [{'alpha': alphas},]\n",
    "n_folds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(lasso, param_grid, cv=n_folds,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = -gs.cv_results_['mean_train_score']\n",
    "train_scores_std = -gs.cv_results_['std_train_score']\n",
    "val_scores = -gs.cv_results_['mean_test_score']\n",
    "val_scores_std = -gs.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10, 6)\n",
    "plt.semilogx(alphas, val_scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = val_scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, val_scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, val_scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, val_scores + std_error, val_scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(val_scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions = best_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, np.abs(best_model.coef_)):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Lasso_Coef'})\n",
    "importances = importances.sort_values(by='Lasso_Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances[importances['Lasso_Coef']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "param_grid = [{'alpha': alphas},]\n",
    "n_folds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(ridge, param_grid, cv=n_folds,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = -gs.cv_results_['mean_train_score']\n",
    "train_scores_std = -gs.cv_results_['std_train_score']\n",
    "val_scores = -gs.cv_results_['mean_test_score']\n",
    "val_scores_std = -gs.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10, 6)\n",
    "plt.semilogx(alphas, val_scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = val_scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, val_scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, val_scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, val_scores + std_error, val_scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(val_scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, np.abs(best_model.coef_)):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Ridge_Coef'})\n",
    "importances = importances.sort_values(by='Ridge_Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances[importances['Ridge_Coef']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Using ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en = linear_model.ElasticNet(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-4, -0.5, 30)\n",
    "param_grid = [{'alpha': alphas},]\n",
    "n_folds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(en, param_grid, cv=n_folds,scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = -gs.cv_results_['mean_train_score']\n",
    "train_scores_std = -gs.cv_results_['std_train_score']\n",
    "val_scores = -gs.cv_results_['mean_test_score']\n",
    "val_scores_std = -gs.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10, 6)\n",
    "plt.semilogx(alphas, val_scores)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = val_scores_std / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, val_scores + std_error, 'b--')\n",
    "plt.semilogx(alphas, val_scores - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, val_scores + std_error, val_scores - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(val_scores), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, np.abs(best_model.coef_)):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'EN_Coef'})\n",
    "importances = importances.sort_values(by='EN_Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances[importances['EN_Coef']>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Regressor Does very well on both Training and Validation - a promising model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30], 'max_features':[2,4,6,8,10,12]},\n",
    "    {'bootstrap': [True,False], 'n_estimators':[3,10], 'max_features':[2,4,6,8,10,12]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_for_reg = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(new_for_reg, param_grid, cv=4, scoring='neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Avg Mean Train Score: ', -gs_results['mean_train_score'].mean()\n",
    "print 'Avg Mean Val Score: ', -gs_results['mean_test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = gs.best_estimator_.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "imp = pd.DataFrame(sorted(zip(X.columns, imp),reverse=True,key=itemgetter(1)), columns=['Feature', 'Importance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.sort_values(by='Importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now testing with Test Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now trying SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv_reg = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'C':[i for i in range(1,20,2)], 'epsilon':[i for i in range(1,18,1)], 'kernel':['linear', 'poly', 'rbf'],\n",
    "    'degree':[2,3,4]},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(sv_reg, param_grid, cv=4, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_results = gs.cv_results_\n",
    "print 'Avg Mean Train Score: ', -gs_results['mean_train_score'].mean()\n",
    "print 'Avg Mean Val Score: ', -gs_results['mean_test_score'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "imp = pd.DataFrame(sorted(zip(X.columns, imp),reverse=True,key=itemgetter(1)), columns=['Feature', 'Importance'])\n",
    "imp.sort_values(by='Importance', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_residuals(X_test,y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_predictions(y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVR did not give satisfactory results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br = BaggingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30], 'max_features':[i for i in range(10,80,10)], 'bootstrap': [True,False]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(new_for_reg, param_grid, cv=4, scoring='neg_mean_absolute_error')\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.best_estimator_\n",
    "gs_results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores = -gs.cv_results_['mean_train_score']\n",
    "train_scores_std = -gs.cv_results_['std_train_score']\n",
    "val_scores = -gs.cv_results_['mean_test_score']\n",
    "val_scores_std = -gs.cv_results_['std_test_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = gs.best_estimator_\n",
    "test_predictions = best_model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_residuals(X_test,y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_predictions(y_test,test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now doing nonlinear regressions with Interaction Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, predictions, r2, mse, mae, rmse = nonlinear_reg(X_train_intns, y_train, 2)\n",
    "test_predictions = model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model, predictions, r2, mse, mae, rmse = nonlinear_reg(X_train_intns, y_train, 3)\n",
    "test_predictions = model.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
