{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Preparation Notebook Consolidating All Data Prep code into linear process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from StringIO import StringIO\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "listings_original = pd.read_csv('../Datasources/inside_airbnb/listings.csv')\n",
    "calendar_original = pd.read_csv('../Datasources/inside_airbnb/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def parse_columns(listings, cols):\n",
    "    chars = \"%$\"\n",
    "    for i in cols:\n",
    "        listings[i] = listings[i].astype(str).map(lambda x: x.rstrip(chars))\n",
    "        listings[i] = listings[i].astype(str).map(lambda x: x.lstrip(chars))\n",
    "        listings[i] = listings[i].apply(pd.to_numeric, errors='coerce')\n",
    "        listings[i].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    return listings        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = parse_columns(listings_original, ['host_response_rate', 'cleaning_fee',\n",
    "                                     'host_acceptance_rate','extra_people',\n",
    "                                     'weekly_price', 'monthly_price', 'security_deposit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function to produce 4 listings dataframes (whole, holiday, wke, wkd) with listing mean price\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
    "\n",
    "def get_mean_price(cal, listings):\n",
    "    \n",
    "    cal['price'] = cal['price'].astype(str).map(lambda x: x.lstrip('$'))\n",
    "    cal['price'] = cal['price'].apply(pd.to_numeric, errors='coerce')\n",
    "    cal['price'].replace(regex=True,inplace=True,to_replace=r'\\D',value=r'')\n",
    "    cal['date'] = pd.to_datetime(cal['date'])\n",
    "    cal['month'] = cal['date'].apply(lambda x: x.month)\n",
    "    cal['day'] = cal['date'].apply(lambda x: x.day)\n",
    "    cal['day_of_week'] = cal['date'].dt.weekday_name\n",
    "    \n",
    "    cl = calendar()\n",
    "    holidays = cl.holidays(start=cal['date'].min(), end=cal['date'].max())\n",
    "    \n",
    "    cal['holiday'] = cal['date'].isin(holidays)\n",
    "    cal = cal[(cal['date']>'2016-07-06')&(cal['date']<'2016-10-06')]\n",
    "    \n",
    "    c = cal.loc[cal.available!='f']\n",
    "    c = c[['listing_id','date','price','month','day_of_week','holiday']]\n",
    "    c=c.fillna(c.mean())\n",
    "    \n",
    "    c_hol = c[c['holiday']==True]\n",
    "    c_wke = c[(c['holiday']==False)&((c['day_of_week']=='Sunday')|(c['day_of_week']=='Saturday'))]\n",
    "    c_wkd = c[(~c.isin(c_hol['date']))&(~c.isin(c_wke['date']))]\n",
    "\n",
    "\n",
    "    price_hol_dict = {'price': c_hol.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_hol.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_hol.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).std(),                 \n",
    "                  'skew_of_price': c_hol.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_hol.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wke_dict = {'price': c_wke.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wke.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wke.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wke.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wke.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_wkd_dict = {'price': c_wkd.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c_wkd.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c_wkd.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c_wkd.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                     'median_price': c_wkd.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "    price_whole_dict = {'price': c.groupby(by='listing_id')['price'].mean(), \n",
    "                  'max_price': c.groupby(by='listing_id')['price'].max(), \n",
    "                  'min_price': c.groupby(by='listing_id')['price'].min(), \n",
    "                  'stdev_of_price': c.groupby(by='listing_id')['price'].fillna(0).std(),\n",
    "                  'skew_of_price': c.groupby(by='listing_id')['price'].fillna(0).skew(),\n",
    "                       'median_price': c.groupby(by='listing_id')['price'].median()}\n",
    "\n",
    "\n",
    "\n",
    "    price_hol = pd.DataFrame(price_hol_dict)\n",
    "    price_wke = pd.DataFrame(price_wke_dict)\n",
    "    price_wkd = pd.DataFrame(price_wkd_dict)\n",
    "    price_c = pd.DataFrame(price_whole_dict)    \n",
    "    \n",
    "    price_hol = price_hol.reset_index()\n",
    "    price_wke = price_wke.reset_index()\n",
    "    price_wkd = price_wkd.reset_index()\n",
    "    price_c = price_c.reset_index()\n",
    "\n",
    "    listings_hol = listings.merge(price_hol, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wke = listings.merge(price_wke, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_wkd = listings.merge(price_wkd, how='inner', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings.merge(price_c, how='inner', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    price_hol_new = price_hol.rename(columns = {'max_price': 'hol_max_price', 'min_price': 'hol_min_price', 'price': 'hol_price',\n",
    "                                           'skew_of_price': 'hol_skew_of_price', 'stdev_of_price': 'hol_stdev_of_price',\n",
    "                                               'median_price' : 'hol_median_price'})\n",
    "    price_wke_new = price_wke.rename(columns = {'max_price': 'wke_max_price', 'min_price': 'wke_min_price', 'price': 'wke_price',\n",
    "                                           'skew_of_price': 'wke_skew_of_price', 'stdev_of_price': 'wke_stdev_of_price',\n",
    "                                               'median_price' : 'wke_median_price'})\n",
    "    price_wkd_new = price_wkd.rename(columns = {'max_price': 'wkd_max_price', 'min_price': 'wkd_min_price', 'price': 'wkd_price',\n",
    "                                           'skew_of_price': 'wkd_skew_of_price', 'stdev_of_price': 'wkd_stdev_of_price',\n",
    "                                               'median_price' : 'wkd_median_price'})\n",
    "    \n",
    "    listings_c = listings_c.merge(price_hol_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings_c.merge(price_wke_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    listings_c = listings_c.merge(price_wkd_new, how='outer', left_on='id', right_on='listing_id')\n",
    "    \n",
    "    L_hol = ['hol_max_price', 'hol_min_price', 'hol_price', 'hol_skew_of_price', 'hol_stdev_of_price', 'hol_median_price']\n",
    "    L_wke = ['wke_max_price', 'wke_min_price', 'wke_price', 'wke_skew_of_price', 'wke_stdev_of_price', 'wke_median_price']\n",
    "    L_wkd = ['wkd_max_price', 'wkd_min_price', 'wkd_price', 'wkd_skew_of_price', 'wkd_stdev_of_price', 'wkd_median_price']\n",
    "    \n",
    "    listings_c[L_hol + L_wke + L_wkd] = listings_c[L_hol + L_wke + L_wkd].fillna(0)\n",
    "    listings_c = listings_c.drop(['listing_id_y'], axis = 1)\n",
    "    listings_c['listing_id_x'] = listings_c['listing_id_x'].fillna(0)\n",
    "    \n",
    "    #len(cal['listing_id'].astype(str).unique())\n",
    "    #count = len(c['listing_id'].astype(str).unique())\n",
    "    \n",
    "    #print('Due to the above filtering on calendar, the right total count of listings is: ' %(count))\n",
    "    \n",
    "    return listings_hol, listings_wke, listings_wkd, listings_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings_hol, listings_wke, listings_wkd, listings = get_mean_price(calendar_original, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from collections import defaultdict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#use n components in place of n topics when using gridsearchcv\n",
    "def create_topics(pdseries, listings):\n",
    "        corpus = pdseries.fillna('none')\n",
    "        \n",
    "        vectorizer = CountVectorizer(analyzer='word',       \n",
    "                             min_df=10,                        # minimum reqd occurences of a word \n",
    "                             stop_words='english',             # remove stop words\n",
    "                             lowercase=True,                   # convert all words to lowercase\n",
    "                             token_pattern='[a-zA-Z0-9]{3,}',  # num chars > 3\n",
    "                             # max_features=50000,             # max number of uniq words\n",
    "                            )\n",
    "        \n",
    "        data_vectorized = vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        lda_model = LatentDirichletAllocation(n_topics=20,               # Number of topics\n",
    "                                      max_iter=10,               # Max learning iterations\n",
    "                                      learning_method='online',   \n",
    "                                      random_state=100,          # Random state\n",
    "                                      batch_size=128,            # n docs in each learning iter\n",
    "                                      evaluate_every = -1,       # compute perplexity every n iters, default: Don't\n",
    "                                      n_jobs = -1,               # Use all available CPUs\n",
    "                                     )\n",
    "        \n",
    "        lda_output = lda_model.fit_transform(data_vectorized)\n",
    "\n",
    "        # column names\n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        topicnames = [str(col_name) + \"-\" + \"Topic\" + str(i) for i in range(lda_model.n_topics)]\n",
    "\n",
    "        # index names\n",
    "        docnames = [str(col_name) + \"-\" + \"Doc\" + str(i) for i in range(len(corpus))]\n",
    "\n",
    "        # Make the pandas dataframe\n",
    "        df_document_topic = pd.DataFrame(np.round(lda_output, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "        # Get dominant topic for each document\n",
    "        dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "        df_document_topic[str(col_name) + \"-\" + 'Dominant_Topic'] = dominant_topic\n",
    "        \n",
    "        df_document_topic.index = [i for i in range(len(df_document_topic))]\n",
    "        \n",
    "        df_document_topic = df_document_topic.fillna(0)\n",
    "        \n",
    "        out = df_document_topic.merge(listings, left_index=True, right_index=True)\n",
    "        out = out.astype('str')\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/decomposition/online_lda.py:294: DeprecationWarning: n_topics has been renamed to n_components in version 0.19 and will be removed in 0.21\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "#removing experiences offered as column was all nulls\n",
    "\"\"\"text_features = ['space', 'description', \n",
    "                 'neighborhood_overview', 'notes', 'transit', \n",
    "                 'access', 'interaction', 'house_rules']\"\"\"\n",
    "#5/23/18\n",
    "#using only description:\n",
    "text_features = ['description']\n",
    "new = listings.copy()\n",
    "for i in text_features:\n",
    "    new = create_topics(listings[i], new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "def create_txt_features(pdseries, listings):\n",
    "    \n",
    "    textLength = []\n",
    "    textWordsPerc = []\n",
    "    textPuncPerc = []\n",
    "    textDigitsPerc = []\n",
    "\n",
    "    for i in pdseries:\n",
    "        tokens = re.findall(r\"[\\w']+|[.,!?;]\", i)\n",
    "        textLength.append(len(tokens))\n",
    "\n",
    "        if len(tokens)==0:\n",
    "            textWordsPerc.append(0)\n",
    "            textPuncPerc.append(0)\n",
    "            textDigitsPerc.append(0)\n",
    "\n",
    "        else:\n",
    "            textWordsPerc.append(len(i.split())/float(len(tokens)))\n",
    "            textPuncPerc.append(len(''.join(c for c in i if c in string.punctuation))/float(len(tokens)))\n",
    "            textDigitsPerc.append(len(''.join(c for c in i if c in string.digits))/float(len(tokens)))\n",
    "\n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    \n",
    "    textLength_varname = str(col_name) + '_TextLength'\n",
    "    textWordsPerc_varname = str(col_name) + '_TextWordsPerc'\n",
    "    textPuncPerc_varname = str(col_name) + '_TextPuncPerc'\n",
    "    textDigitsPerc_varname = str(col_name) + '_TextDigitsPerc'\n",
    "    \n",
    "    listings[textLength_varname] = textLength\n",
    "    listings[textWordsPerc_varname] = textWordsPerc\n",
    "    listings[textPuncPerc_varname] = textPuncPerc\n",
    "    listings[textDigitsPerc_varname] = textDigitsPerc\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new2 = new.copy()\n",
    "for i in text_features:\n",
    "    new2 = create_txt_features(new[i], new2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lexical_diversity(pdseries, listings):\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_LexicalDiversity\"\n",
    "    \n",
    "    lx_div = pd.Series([len(i)/len(set(i)) for i in pdseries])\n",
    "    listings[varname] = lx_div\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new3 = new2.copy()\n",
    "for i in text_features:\n",
    "    new3 = lexical_diversity(new2[i], new3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_grammar(pdseries, listings):\n",
    "    \n",
    "    import nltk\n",
    "    from nltk.tag import pos_tag, map_tag\n",
    "    from collections import Counter\n",
    "      \n",
    "    df = pd.DataFrame()\n",
    "    for text in pdseries:\n",
    "        \n",
    "        col_name = pd.DataFrame(pdseries).columns[0]\n",
    "        \n",
    "        \n",
    "        tokenized_text = nltk.word_tokenize(text.decode('utf-8'))\n",
    "        grammar = [i[1] for i in nltk.pos_tag(tokenized_text, tagset='universal')]\n",
    "        \n",
    "        counter = Counter(grammar)\n",
    "        fr = pd.DataFrame(counter, index=[0])\n",
    "        fr.columns = [str(col_name) + '_' + str(i) for i in fr.columns]\n",
    "        \n",
    "        fr2 = fr/len(tokenized_text)\n",
    "        fr2.columns = [str(i) + '_tokens_sum_ratio' for i in fr2.columns]\n",
    "        \n",
    "        fr3 = pd.concat([fr, fr2], ignore_index=True)\n",
    "        \n",
    "        df = pd.concat([df, fr3], ignore_index=True)\n",
    "        \n",
    "        \n",
    "        \n",
    "    df = df.fillna(0)\n",
    "        \n",
    "    return listings.merge(df, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new4 = new3.copy()\n",
    "for i in text_features:\n",
    "    new4 = extract_grammar(new3[i], new4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "def kmeans_Clusterer(pdseries, listings):\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(pdseries)\n",
    "    true_k = 10\n",
    "    model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n",
    "    model.fit(X)\n",
    "    \n",
    "    col_name = pd.DataFrame(pdseries).columns[0]\n",
    "    varname = str(col_name) + \"_KmeansCluster\"\n",
    "    \n",
    "    listings[varname] = pd.Series(model.labels_)\n",
    "    listings[varname] = listings[varname].fillna(0)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new5 = new4.copy()\n",
    "for i in text_features:\n",
    "    new5 = kmeans_Clusterer(new4[i], new5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def string_to_set(x):\n",
    "    c = set()\n",
    "    for w in x[1:-1].split(\",\"):\n",
    "        c.add(w)\n",
    "        \n",
    "    return c\n",
    "\n",
    "def has_amenity(x, amen_):\n",
    "    if amen_ in x:\n",
    "        return 1\n",
    "    pass\n",
    "\n",
    "def boolean_convert(z):\n",
    "    if z:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_amenities(listings):\n",
    "    \n",
    "    a = listings['amenities'].fillna('{}')\n",
    "    listings['amenities_set'] = a.map(string_to_set)\n",
    "    all_amenities = set()\n",
    "    \n",
    "    for idx in listings['amenities_set'].index:\n",
    "        all_amenities = all_amenities.union(listings['amenities_set'][idx])\n",
    "        \n",
    "    b = set(['', '\"translation missing: en.hosting_amenity_49\"','\"translation missing: en.hosting_amenity_50\"'])\n",
    "    \n",
    "    all_amenities = all_amenities.difference(b)\n",
    "    \n",
    "    for amen in all_amenities:\n",
    "        \n",
    "        b = listings['amenities_set'].map(lambda x: amen in x).map(boolean_convert)\n",
    "        \n",
    "        if len(amen.split(' ')) == 1:\n",
    "            listings['has_' + amen] = b\n",
    "            continue\n",
    "            \n",
    "        if \"\" in amen:\n",
    "            amen = amen[1:-1].replace(' ', '_')\n",
    "            \n",
    "        listings['has_' + amen] = b\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = new5.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new6 = add_amenities(new6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_host_verifications(listings):\n",
    "    a = listings['host_verifications'].map(lambda x: x[1:-1]).map(lambda j: j.split(',')).map(lambda k: set(k))\n",
    "    all_host_verifications = set()\n",
    "    \n",
    "    for w in a.index:\n",
    "        all_host_verifications = all_host_verifications.union(a[w])\n",
    "    \n",
    "    for w in all_host_verifications:\n",
    "        \n",
    "        b = a.map(lambda x: w in x).map(boolean_convert)\n",
    "        \n",
    "        if '' in w:\n",
    "            w = w.strip()[1:-1].replace(' ', '_')\n",
    "            \n",
    "        listings['uses_' + w] = b\n",
    "    \n",
    "    return listings      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new7 = new6.copy()\n",
    "new7 = add_host_verifications(new7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#def new_add_distance_from_ocean(listings):\n",
    "    #lst2 = []\n",
    "    #for i,k in zip(new7['latitude'],new7['longitude']):\n",
    "        #lon_diff = (float(k) + 117.235585)*np.pi/180\n",
    "        #lat_diff = (float(i) - 32.802458)*np.pi/180\n",
    "        #a = np.sin(lat_diff/2)**2 + np.cos(float(i)*np.pi/180)*np.cos(32.802458*np.pi/180)*(np.sin(lon_diff/2)**2)\n",
    "        #c = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        #d = 6371.00*float(c)\n",
    "        #lst2.append(d)\n",
    "        \n",
    "    #listings['distance_from_ocean'] = lst2\n",
    "    \n",
    "    #return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "contour_list = [(-117.12542727272728, 32.50603636363636),(-117.1239, 32.52068181818182),(-117.12237272727273, 32.53532727272727),(-117.12785, 32.550577272727274),\n",
    " (-117.13332727272727, 32.565827272727276),(-117.14155757575757, 32.58140454545455),(-117.14978787878788, 32.59698181818182),(-117.15801818181818, 32.61255909090909),\n",
    " (-117.14722727272728, 32.6186),(-117.13142727272728, 32.61995454545455), (-117.11562727272728, 32.621309090909094),(-117.16624848484848, 32.628136363636365),(-117.11614090909092, 32.6319),\n",
    " (-117.15394090909092, 32.632684090909095),(-117.11665454545455, 32.64249090909091),(-117.17447878787878, 32.643713636363636),(-117.16065454545455, 32.64676818181818),(-117.12490000000001, 32.6552),\n",
    " (-117.2549909090909, 32.65713636363636),(-117.18270909090909, 32.659290909090906), (-117.16736818181818, 32.66085227272727),(-117.24204545454546, 32.66253636363636),\n",
    " (-117.19277575757576, 32.66720303030303),(-117.13314545454546, 32.66790909090909),(-117.26173636363637, 32.67278181818182),(-117.24129545454545, 32.673590909090905),\n",
    " (-117.17408181818182, 32.67493636363636),(-117.20284242424242, 32.67511515151515),(-117.14139090909092, 32.68061818181818), (-117.2129090909091, 32.68302727272727),(-117.24054545454545, 32.68464545454545),\n",
    " (-117.18729545454545, 32.686263636363634), (-117.26848181818183, 32.688427272727274), (-117.15354363636364, 32.690034545454544),\n",
    " (-117.21238181818183, 32.69705454545455), (-117.2005090909091, 32.69759090909091), (-117.26729545454546, 32.698513636363636),\n",
    " (-117.16569636363637, 32.699450909090906), (-117.23467727272728, 32.700131818181816), (-117.2661090909091, 32.7086),\n",
    " (-117.17784909090909, 32.708867272727275), (-117.2288090909091, 32.71561818181818), (-117.19000181818183, 32.71828363636364),\n",
    " (-117.21548181818181, 32.721659090909085), (-117.26427272727273, 32.72405454545454), (-117.20215454545455, 32.7277), (-117.26243636363637, 32.73950909090909),\n",
    " (-117.26060000000001, 32.754963636363634), (-117.25876363636364, 32.77041818181818), (-117.26415909090909, 32.78511818181818),\n",
    " (-117.26955454545455, 32.79981818181818), (-117.27495, 32.81451818181819), (-117.28034545454545, 32.829218181818185), (-117.27120303030303, 32.83975454545455),\n",
    " (-117.26206060606061, 32.85029090909091), (-117.25291818181819, 32.86082727272727), (-117.24906363636364, 32.87129090909091),\n",
    " (-117.24953181818182, 32.88050454545454), (-117.25, 32.88971818181818), (-117.25397272727272, 32.90718909090909), (-117.25794545454545, 32.92466),\n",
    " (-117.26191818181819, 32.942130909090906), (-117.26589090909091, 32.95960181818182), (-117.26986363636364, 32.97707272727273),\n",
    " (-117.27609772727273, 32.993097727272726), (-117.28233181818182, 33.009122727272725), (-117.2885659090909, 33.025147727272724),\n",
    " (-117.29480000000001, 33.04117272727272), (-117.3010340909091, 33.05719772727273), (-117.30726818181819, 33.07322272727273), (-117.31350227272728, 33.08924772727273)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def search_for_min_index(L, x):\n",
    "    B = []\n",
    "    count = 0\n",
    "    for i, j in L:\n",
    "        J = float(j)\n",
    "        X = float(x)\n",
    "        b = abs(J - X)\n",
    "        B.append((count, b))\n",
    "        count = count + 1\n",
    "        \n",
    "    B = sorted(B, key = lambda x: x[1])\n",
    "    return (B[0][0], B[1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_add_distance_from_ocean(listings):\n",
    "    lst2 = []\n",
    "    for i,k in zip(new7['latitude'],new7['longitude']):\n",
    "        j_one, j_two = search_for_min_index(contour_list, i)\n",
    "        val_one_i, val_one_ii = contour_list[j_one]\n",
    "        val_two_i, val_two_ii = contour_list[j_two]\n",
    "        val_ii_max = max(val_one_ii, val_two_ii)\n",
    "        val_ii_min = min(val_one_ii, val_two_ii)\n",
    "        \n",
    "        #rat = (i - val_ii_min)/val_ii_max\n",
    "        #val_i_max = max(val_one_i, val_two_i)\n",
    "        #val_i_min = min(val_one_i, val_two_i)\n",
    "        #val_i = val_i_min*rat + val_i_max*(1 - rat)\n",
    "        \n",
    "        lon_diff = (float(k) - val_one_i)*np.pi/180\n",
    "        lat_diff = (float(i) - val_one_ii)*np.pi/180\n",
    "        a = np.sin(lat_diff/2)**2 + np.cos(float(i)*np.pi/180)*np.cos(val_one_ii*np.pi/180)*(np.sin(lon_diff/2)**2)\n",
    "        c_one = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        d_one = 6371.00*float(c_one)\n",
    "        \n",
    "        lon_diff = (float(k) - val_two_i)*np.pi/180\n",
    "        lat_diff = (float(i) - val_two_ii)*np.pi/180\n",
    "        a = np.sin(lat_diff/2)**2 + np.cos(float(i)*np.pi/180)*np.cos(val_two_ii*np.pi/180)*(np.sin(lon_diff/2)**2)\n",
    "        c_two = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        d_two = 6371.00*float(c_two)\n",
    "        \n",
    "        #lon_diff = (float(k) - val_i)*np.pi/180\n",
    "        #a = (np.cos(float(i)*np.pi/180)**2)*(np.sin(lon_diff/2)**2)\n",
    "        #c_three = np.arctan2(np.sqrt(a), np.sqrt(1 - a))\n",
    "        #d_three = 6371.00*float(c_three)\n",
    "        #d = min(d_one, d_two, d_three)\n",
    "        \n",
    "        d = min(d_one, d_two)\n",
    "        lst2.append(d)\n",
    "        \n",
    "    listings['distance_from_ocean'] = lst2\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new8 = new7.copy()\n",
    "new8 = new_add_distance_from_ocean(new8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def street_zipcode_parse(x):\n",
    "    st = ''\n",
    "    \n",
    "    if 'Mexico' in x:\n",
    "            return '00000.0'\n",
    "    \n",
    "    for w in x:\n",
    "        try:\n",
    "            a = int(w)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        \n",
    "        st = st + w\n",
    "        \n",
    "    return (st + '.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Getting the cleansed zipcode\n",
    "new8['zipcode_cleansed'] = new8['street'].map(street_zipcode_parse).map(lambda x: x[-7:]).astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoder(listings, encoded_features):\n",
    "    \n",
    "    label_enc = LabelEncoder()\n",
    "    \n",
    "    for col in encoded_features:\n",
    "        \n",
    "        listings[col] = listings[col].astype(str)\n",
    "        \n",
    "        var_name = str(col) + '_enc'\n",
    "        listings[var_name] = label_enc.fit_transform(listings[col])\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_vars = ['host_response_time', 'calendar_updated', 'bed_type', 'jurisdiction_names', 'zipcode',\n",
    "               'cancellation_policy', 'zipcode_cleansed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new9 = new8.copy()\n",
    "new9 = encoder(new9, encoded_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Caution!!! The input features are not dropped by the following to columns - they must be dropped as part of modeling\n",
    "#5/23/18 the unwanted columns are detected and dropped in the featureExplorastio notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binarizer(listings, binarized_features):\n",
    "    \n",
    "    label_enc = LabelBinarizer()\n",
    "    \n",
    "    for col in binarized_features:\n",
    "        \n",
    "        listings[col] = listings[col].astype(str)\n",
    "        \n",
    "        var_name = str(col) + '_bin'\n",
    "        listings[var_name] = label_enc.fit_transform(listings[col])\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "binarized_vars = ['host_is_superhost','is_location_exact','host_has_profile_pic','host_identity_verified',\n",
    "                  'instant_bookable','require_guest_profile_picture','require_guest_phone_verification']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new10 = new9.copy()\n",
    "new10 = binarizer(new10, binarized_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#takes list of features that should be numeric and transforms them to float\n",
    "#Also takes care of the topic features - these need not be input into the features parameter\n",
    "def make_numeric(listings):\n",
    "    #Taking Care of topics features\n",
    "    topic_cols = listings.filter(regex='Topic').columns\n",
    "    listings[topic_cols] = listings[topic_cols].astype(float)\n",
    "    \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11 = new10.copy()\n",
    "new11 = make_numeric(new11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11 = new11.rename(columns = {'listing_id_x': 'id'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "col_counts = defaultdict(int)\n",
    "col_ix = new11.first_valid_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:2: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "cols = []\n",
    "for col in new11.ix[col_ix].index:\n",
    "    cnt = col_counts[col]\n",
    "    col_counts[col] += 1\n",
    "    suf = '_' + str(cnt) if cnt else ''\n",
    "    cols.append(col + suf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11.columns = cols\n",
    "new11 = new11.drop([col_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Keep the below line just in case\n",
    "#new11 = new11.drop(columns= ['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', \n",
    "               'beds', 'guests_included', 'minimum_nights',\n",
    "               'maximum_nights', 'availability_30', 'availability_60','availability_90',\n",
    "               'availability_365', 'number_of_reviews', 'review_scores_rating', 'review_scores_accuracy',\n",
    "               'review_scores_cleanliness', 'review_scores_checkin', 'review_scores_communication',\n",
    "               'review_scores_location', 'review_scores_value', 'calculated_host_listings_count', \n",
    "               'reviews_per_month', 'max_price','median_price','min_price','price_y','skew_of_price',\n",
    "                'stdev_of_price','hol_max_price','hol_median_price','hol_min_price','hol_price',\n",
    "                'hol_skew_of_price','hol_stdev_of_price','wke_max_price','wke_median_price',\n",
    "                'wke_min_price','wke_price','wke_skew_of_price','wke_stdev_of_price','wkd_max_price',\n",
    "                'wkd_median_price','wkd_min_price','wkd_price','wkd_skew_of_price','wkd_stdev_of_price', 'id',\n",
    "               'host_listings_count', 'host_total_listings_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11 = parse_columns(new11,num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events = pd.read_csv('../Datasources/listings_events/listings_events_2018-05-29_V1.csv')\n",
    "parks = pd.read_csv('../Datasources/listings_parks/listings_parks_2018-05-29_V1.csv')\n",
    "#add businesses file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def take_equality(x):\n",
    "    if x == 0:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events['all_events_within_1_km_at_min_distance'] = (events['event_count_1km'] - events['count_of_events_at_min_distance']).map(take_equality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "events['all_events_within_3_km_at_min_distance'] = (events['event_count_3km'] - events['count_of_events_at_min_distance']).map(take_equality)\n",
    "events['all_events_within_5_km_at_min_distance'] = (events['event_count_5km'] - events['count_of_events_at_min_distance']).map(take_equality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new11 = new11.merge(events, how = 'inner', left_on = 'id', right_on = 'listing_id')\n",
    "new11 = new11.merge(parks, how = 'inner', left_on = 'id', right_on = 'listing_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_enc_vars = ['closest_park_full_name', 'park_type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6608, 9)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#EVENTS HAS WAY TOO MANY ROWS -- FIX\n",
    "events.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6608, 8)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new12 = new11.copy()\n",
    "new12 = encoder(new12, new_enc_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_bin_vars = ['all_events_within_1_km_at_min_distance', 'all_events_within_3_km_at_min_distance', 'all_events_within_5_km_at_min_distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new12 = binarizer(new12, new_bin_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def datetime_strip(x):\n",
    "    x_date = datetime.datetime.strptime(x, \"%Y-%m-%d\")\n",
    "    return x_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_days(b):\n",
    "    return b.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_days_since_host(listings):\n",
    "    listings['days_since_host'] = (listings['last_scraped'].map(datetime_strip) - listings['host_since'].map(datetime_strip)).map(get_days)\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new13 = new12.copy()\n",
    "new13 = add_days_since_host(new13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calendar_update_parse(x):\n",
    "    if x == 'today':\n",
    "        return 0\n",
    "    \n",
    "    elif x == 'yesterday':\n",
    "        return 1    \n",
    "    \n",
    "    else:        \n",
    "        b = x.split(' ')\n",
    "        \n",
    "        if 'days' in b:\n",
    "            a = int(b[0])\n",
    "            return a\n",
    "        \n",
    "        elif 'week' in b:\n",
    "            return 7\n",
    "        \n",
    "        elif 'weeks' in b:\n",
    "            a = 7*int(b[0])\n",
    "            return a\n",
    "        \n",
    "        elif 'month' in b:\n",
    "            a = 365.25/12.0\n",
    "            return np.floor(a)\n",
    "        \n",
    "        elif 'months' in b:\n",
    "            a = (365.25*float(b[0]))/12.0\n",
    "            return np.floor(a)\n",
    "    pass\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_calendar_updated_cleaned(listings):\n",
    "    listings[\"calendar_updated_numeric\"] = listings[\"calendar_updated\"].map(calendar_update_parse)\n",
    "    listings[\"calendar_updated_numeric\"] = listings[\"calendar_updated_numeric\"].fillna(listings[\"calendar_updated_numeric\"].max())\n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new14 = new13.copy()\n",
    "new14 = add_calendar_updated_cleaned(new14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "high_quality_amenities_list = ['has_Other_pet(s)', 'has_Elevator_in_Building', 'has_Indoor_Fireplace', 'has_Buzzer/Wireless_Intercom',\n",
    " 'has_Gym', 'has_Hot_Tub', 'has_Suitable_for_Events', 'has_Cat(s)', 'has_Pets_live_on_this_property', 'has_Safety_Card','has_Smoking_Allowed',\n",
    " 'has_Pool', 'has_Pets_Allowed', 'has_Wheelchair_Accessible', 'has_Dog(s)', 'has_Breakfast', 'has_Doorman', 'has_Lock_on_Bedroom_Door']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mid_quality_amenities_list = ['has_Fire_Extinguisher', 'has_Cable_TV', 'has_Air_Conditioning', 'has_Hair_Dryer',\n",
    " 'has_Iron', 'has_First_Aid_Kit', 'has_Laptop_Friendly_Workspace', 'has_24-Hour_Check-in']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_quality_amenities_list = ['has_Essentials', 'has_Carbon_Monoxide_Detector', 'has_Internet', 'has_Washer', 'has_Hangers',\n",
    " 'has_TV', 'has_Kitchen', 'has_Family/Kid_Friendly', 'has_Shampoo', 'has_Heating', 'has_Smoke_Detector', 'has_Free_Parking_on_Premises',\n",
    " 'has_Dryer', 'has_Wireless_Internet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def listings_quality_classification(listings):\n",
    "    listings['amenity_level'] = ''\n",
    "        \n",
    "    for w in listings.index:\n",
    "        \n",
    "        for j in high_quality_amenities_list:\n",
    "            a = listings[j][w]\n",
    "            \n",
    "            if a == 1:\n",
    "                listings['amenity_level'][w] = 'high'\n",
    "                break \n",
    "                \n",
    "        if listings['amenity_level'][w] == 'high':\n",
    "            continue\n",
    "                \n",
    "        for k in mid_quality_amenities_list:\n",
    "            a = listings[k][w]\n",
    "            \n",
    "            if a == 1:\n",
    "                listings['amenity_level'][w] = 'mid_level'\n",
    "                break\n",
    "                \n",
    "        if listings['amenity_level'][w] == 'mid_level':\n",
    "            continue\n",
    "            \n",
    "        else:\n",
    "            listings['amenity_level'][w] = 'low'\n",
    "            \n",
    "    return listings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/ipykernel_launcher.py:27: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "new15 = new14.copy()\n",
    "new15 = listings_quality_classification(new15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_make_num = ['uses_phone', 'uses_manual_online','has_Pets_Allowed','has_Wheelchair_Accessible',\n",
    "               'has_First_Aid_Kit', 'has_Cat(s)','has_Pets_Allowed', 'has_24-Hour_Check-in',\n",
    "               ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new15 = parse_columns(new15,new_make_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_enc = ['amenity_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new16 = new15.copy()\n",
    "new16 = encoder(new16, new_enc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Listings Shape at Each Iteration\n",
      "(6608, 95)\n",
      "(5753, 121)\n",
      "(5753, 142)\n",
      "(5753, 146)\n",
      "(5753, 147)\n",
      "(5753, 171)\n",
      "(5753, 172)\n",
      "(5753, 213)\n",
      "(5753, 225)\n",
      "(5753, 227)\n",
      "(5753, 234)\n",
      "(5753, 241)\n",
      "(5752, 258)\n",
      "(5752, 263)\n",
      "(5752, 264)\n",
      "(5752, 265)\n",
      "(5752, 266)\n",
      "(5752, 267)\n"
     ]
    }
   ],
   "source": [
    "print \"Listings Shape at Each Iteration\"\n",
    "print listings_original.shape\n",
    "print listings.shape\n",
    "print new.shape\n",
    "print new2.shape\n",
    "print new3.shape\n",
    "print new4.shape\n",
    "print new5.shape\n",
    "print new6.shape\n",
    "print new7.shape\n",
    "print new8.shape\n",
    "print new9.shape\n",
    "print new10.shape\n",
    "print new11.shape\n",
    "print new12.shape\n",
    "print new13.shape\n",
    "print new14.shape\n",
    "print new15.shape\n",
    "print new16.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Make sure to uncomment and update the count variable whenever needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "today = datetime.date.today()\n",
    "count+=1\n",
    "filename = '../Datasources/listings_augmented/listings_augmented_' + str(today) + '_V' + str(count) + '.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasources/listings_augmented/listings_augmented_2018-06-03_V1.csv\n"
     ]
    }
   ],
   "source": [
    "print filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new16.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
