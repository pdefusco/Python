{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook putting together concepts from all modeling notebook to construct final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "import matplotlib.cm as cm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.metrics import r2_score, mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import KFold,cross_val_predict,cross_val_score, cross_validate, train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder, LabelBinarizer, PolynomialFeatures, MinMaxScaler, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing\n",
    "from sklearn.neighbors import LocalOutlierFactor, KNeighborsRegressor\n",
    "from sklearn.feature_selection import RFE, f_regression, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('./lib')\n",
    "from airbnb_modeling import detect_feature_importance, scale_data, normalize_data, eval_metrics, plot_residuals, plot_predictions\n",
    "from parse_methods import parse_columns\n",
    "from airbnb_modeling import detect_interactions, add_interactions, map_variable, plot_rmse_instances,plot_rmse_features, plot_accuracy_instances\n",
    "#from filename import methodname\n",
    "#from airbnb_modeling import\n",
    "#from airbnb_modeling import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "listings = pd.read_csv('Datasources/listings_augmented/listings_augmented_2018-05-31_V3.csv',low_memory=False)\n",
    "listings = listings.drop(listings.index[4323:4325])\n",
    "listings.index = [i for i in range(len(listings))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Excluded variables from the featuresExploration notebook\n",
    "%store -r excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = [i for i in listings.columns if i not in excluded]\n",
    "X = listings[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "space        1275\n",
       "bathrooms      10\n",
       "bedrooms        3\n",
       "beds            4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[X.columns[X.isnull().any()]].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X.fillna(X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = parse_columns(X, ['has_Pets_Allowed','has_Wheelchair_Accessible','has_First_Aid_Kit',\n",
    "'has_Cat(s)','has_24-Hour_Check-in','uses_jumio','description-Topic0','description-Topic1',\n",
    "'description-Topic4','description-Topic5','description-Topic6','description-Topic10',\n",
    "'description-Topic11','description-Topic12','description-Topic13','description-Topic15',\n",
    "'description-Topic17','description-Topic18','description-Dominant_Topic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X[[i for i in X.columns if i not in X.filter(regex='enc').columns]]\n",
    "donotscale = X.filter(regex='bin').columns\n",
    "cols = [i for i in X.columns if i not in donotscale]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols.remove('space')\n",
    "cols.remove('amenity_level')\n",
    "cols.remove('hol_skew_of_price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "helper_df = pd.DataFrame(preprocessing.normalize(X[cols]), columns = cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_normed = helper_df.merge(X[donotscale], right_index=True, left_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now adding new features by taking ratios between features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "other = ['calculated_host_listings_count','extra_people', 'minimum_nights', 'number_of_reviews']\n",
    "candidates = list(X_normed.filter(regex='event').columns) \\\n",
    "+ list(X_normed.filter(regex='park').columns) + list(X_normed.filter(regex='ocean').columns)\\\n",
    "+ list(X_normed.filter(regex='ratio').columns) + other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "a = []\n",
    "for subset in itertools.combinations(candidates, 2):\n",
    "    a.append(subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new = [i for i in a if \"bin\" not in i[0] and \"bin\" not in i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_ratios = X_normed.copy()\n",
    "for i in new:\n",
    "    name = str(i[0]) + '/' + str(i[1]) + '_ratio'\n",
    "    X_ratios[name] = X_ratios[i[0]]/X_ratios[i[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Series([], dtype: float64)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_normed[X_normed.columns[X_normed.isnull().any()]].isnull().sum().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Lots of nulls above so dropping columns that have more than 300 nulls and imputing the remaining ones\n",
    "X_ratios = X_ratios.dropna(axis = 1,thresh = len(X_normed)-300)\n",
    "X_ratios = X_ratios.replace([np.inf, -np.inf], np.nan)\n",
    "X_ratios = X_ratios.fillna(X_ratios.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_normed = X_normed['price_y'].fillna(X_normed['price_y'].mean())\n",
    "X_normed = X_normed[X_normed.columns.drop(X_normed[list(X_normed.filter(regex='price'))])]\n",
    "X_ratios = X_ratios[X_ratios.columns.drop(X_ratios[list(X_ratios.filter(regex='price'))])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "#Checking that features are clean - output should be True and False\n",
    "print np.any(np.isnan(X_normed))\n",
    "print np.all(np.isfinite(X_normed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step1: Simple Model with Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Number of Features Used:  138\n"
     ]
    }
   ],
   "source": [
    "print 'Final Number of Features Used: ', len(X_train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE for Initial Linear Regression:  0.025984902976912066\n"
     ]
    }
   ],
   "source": [
    "lin_reg_rmse_test = np.sqrt(mean_squared_error(y_test, lin_reg.predict(X_test)))\n",
    "print 'Test RMSE for Initial Linear Regression: ', lin_reg_rmse_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree_reg = DecisionTreeRegressor()\n",
    "tree_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_lin = cross_validate(lin_reg, X_train, y_train, cv=10, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_tree = cross_validate(tree_reg, X_train, y_train, cv=10, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics for Linear Regression with CV: \n",
      "Training R2 Mean:  0.8367261556254737\n",
      "Validation R2 Mean:  0.8141108210063563\n",
      "Validation R2 STdev:  0.02281127611849167\n",
      "--\n",
      "Training RMSE Mean:  0.02517032237136816\n",
      "Validation RMSE Mean:  0.02671513027489586\n",
      "Validation RMSE STdev:  6.635340961825122e-05\n",
      "--\n",
      "Training MAE Mean:  0.0195095861192613\n",
      "Validation MAE Mean:  0.02054359368106534\n",
      "Validation MAE STdev:  0.0009910646197029544\n",
      "----\n",
      "----\n",
      "Evaluation Metrics for Tree Regression with CV: \n",
      "Training R2 Mean:  0.9999977308530943\n",
      "Validation R2 Mean:  0.7280341155808799\n",
      "Validation R2 STdev:  0.04282676111959295\n",
      "--\n",
      "Training RMSE Mean:  9.383340186128147e-05\n",
      "Validation RMSE Mean:  0.032249129556746185\n",
      "Validation RMSE STdev:  0.00010680351130090479\n",
      "--\n",
      "Training MAE Mean:  4.0247711899583353e-05\n",
      "Validation MAE Mean:  0.02271328846210817\n",
      "Validation MAE STdev:  0.0011397584398381844\n"
     ]
    }
   ],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with CV: '\n",
    "eval_metrics(scores_lin)\n",
    "print '----'\n",
    "print '----'\n",
    "print 'Evaluation Metrics for Tree Regression with CV: '\n",
    "eval_metrics(scores_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-9dd2b184fb80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_rmse_instances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlin_reg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/pauldefusco/Documents/capstone/UCSD-MAS-Capstone-Project/dev/lib/airbnb_modeling.pyc\u001b[0m in \u001b[0;36mplot_rmse_instances\u001b[0;34m(clf, X_train, y_train)\u001b[0m\n\u001b[1;32m    373\u001b[0m         cv_results = cross_validate(clf,X_train[:i],y_train[:i],return_train_score=True,\n\u001b[1;32m    374\u001b[0m                                    \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m                                    cv=cv_n)\n\u001b[0m\u001b[1;32m    376\u001b[0m         \u001b[0mtrain_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0mval_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mcv_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test_score'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36mcross_validate\u001b[0;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score)\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0mfit_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_train_score\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m             return_times=True)\n\u001b[0;32m--> 206\u001b[0;31m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/_parallel_backends.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/externals/joblib/parallel.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/model_selection/_validation.pyc\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, error_score)\u001b[0m\n\u001b[1;32m    456\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/sklearn/linear_model/base.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    507\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_residues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m                 \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/pauldefusco/anaconda2/envs/py27/lib/python2.7/site-packages/scipy/linalg/basic.pyc\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[1;32m   1204\u001b[0m             v, x, s, rank, work, info = lapack_func(a1, b1, cond, lwork,\n\u001b[1;32m   1205\u001b[0m                                                     \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1206\u001b[0;31m                                                     overwrite_b=overwrite_b)\n\u001b[0m\u001b[1;32m   1207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1208\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdriver\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gelsd'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "plot_rmse_instances(lin_reg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_rmse_instances(tree_reg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same for more models and create final plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Using Nested GridSearch CV with more regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sv_reg = SVR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Cs = np.logspace(-0.1, 10, 30)\n",
    "param_grid = [\n",
    "    {'C':Cs, 'epsilon':[i for i in range(1,8,1)], 'kernel':['linear', 'poly'],\n",
    "    'degree':[2,3,4]},]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(sv_reg, param_grid, cv=4, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_sv_reg = cross_validate(gs, X_train, y_train, cv=10, return_train_score=True,\n",
    "                         scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_results_svreg = gs.cv_results_\n",
    "print 'Avg Mean Train Score: ', np.sqrt(-gs_results_svreg['mean_train_score'].mean())\n",
    "print 'Avg Mean Val Score: ', np.sqrt(-gs_results_svreg['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_svr = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_rmse_instances(best_model_svr, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K Neighbors Regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "neigh_reg = KNeighborsRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_neighbors':[2,3,4], \n",
    "     'metric':['euclidean']},]\n",
    "#euclidean: sqrt(sum((x - y)^2)) \n",
    "#minkowski: sum(|x - y|^p)^(1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(neigh_reg, param_grid, cv=3, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_neigh_reg = cross_validate(gs, X_train, y_train, cv=3, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_results_nn = gs.cv_results_\n",
    "print 'Avg Mean Train Score: ', np.sqrt(-gs_results_nn['mean_train_score'].mean())\n",
    "print 'Avg Mean Val Score: ', np.sqrt(-gs_results_nn['mean_test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_kneigh = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes way too long\n",
    "#plot_rmse_instances(best_model_kneigh, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Interactions - We do this in two ways - first we check and add the interactions that increase accuracy beyond an arbitrary threshold (0.02); Then, we just add all interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "increments = detect_interactions(X_normed,y_normed, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_normed_wint = add_interactions(X_normed, increments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "increments.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above did not return any interactions! That means the interactions are probably not going to make large changes to our models above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(interaction_only=True)\n",
    "X_normed_wint = poly.fit_transform(X_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed_wint,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg_intonly = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg_intonly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_lin_intonly = cross_validate(lin_reg_intonly, X_train, y_train, cv=10, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with CV - Interactions Only Added: '\n",
    "eval_metrics(scores_lin_intonly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#No need for plot here\n",
    "#plot_rmse_instances(lin_reg_intonly, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding interactions simply overfits. Let's also try adding quadratic terms to the normalized dataset to see if nonlinaer regression might help, although it will probably also overfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "X_normed_quad = poly.fit_transform(X_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed_quad,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "quad_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "quad_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_quad = cross_validate(quad_reg, X_train, y_train, cv=5, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with CV - Interactions Only Added: '\n",
    "eval_metrics(scores_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_rmse_instances(quad_reg, X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite all the attempts, the most promising model is Linear Regression. Let's try the same Linear Regression with the ratios features. We are probably going to overfit again but we will then do Feature Selection: the goal is to choose a good feature subset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores_lin_ratios = cross_validate(lin_reg, X_train, y_train, cv=10, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with CV: '\n",
    "eval_metrics(scores_lin_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "temp_pred = lin_reg.predict(X_test)\n",
    "test_lin_ratios_nofs = np.sqrt(-mean_squared_error(y_test, temp_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model above is promising: the training RMSE has increased more than the validation RMSE, which is a sign that we may be approaching the sweet spot and getting close to starting to overfit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Selection - We use this promising model above to pick only the best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector = RFECV(lin_reg, step=1, cv=5, scoring='neg_mean_squared_error')\n",
    "selector.fit(X_ratios, y_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Optimal number of features : %d\" % selector.n_features_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation RMSE score\")\n",
    "plt.title(\"Optimal number of features : %d\" % selector.n_features_)\n",
    "plt.plot(range(1, len(selector.grid_scores_) + 1), np.sqrt(-selector.grid_scores_))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_new = selector.transform(X_ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All features have equally important ranking!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selector.ranking_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rerun our best model so far and evaluate changes to model metrics resulting from removing unneeded features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg.fit(X_train,y_train)\n",
    "scores_temp = cross_validate(lin_reg, X_train, y_train, cv=10, return_train_score=True,\n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))\n",
    "print 'Evaluation Metrics for Linear Regression with CV: '\n",
    "eval_metrics(scores_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation RMSE has actually increased, while the accuracy has decreased slightly - this is bad but we make the choice to trade off a little bit of accuracy in order to shed complexity (we are losing more than 100 features this way)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now we rebuild the model only with the important features i.e. number of features where val error is lowest - we do this in order to replicate what we created above with RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_normed.columns, selector.ranking_):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'RFECV_Ranking'})\n",
    "importances = importances.sort_values(by='RFECV_Ranking')\n",
    "importances.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are restricting our features to the ones that minimize CV RMSE by feeding the new data into RFECV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_features = list(importances.head(selector.n_features_).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg.fit(X_train[best_features],y_train)\n",
    "scores_lin_ratios_fsel = cross_validate(lin_reg, X_train[best_features], y_train, cv=10, \n",
    "                         scoring=('r2', 'neg_mean_squared_error','neg_mean_absolute_error'))\n",
    "print 'Evaluation Metrics for Linear Regression with CV: '\n",
    "eval_metrics(scores_lin_ratios_fsel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_rmse_features(lin_reg, X_train, y_train, best_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's see how the models performs against our Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_intonly, X_test_intonly, y_train_intonly, y_test_intonly = train_test_split(X_normed_wint,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_quad, X_test_quad, y_train_quad, y_test_quad = train_test_split(X_normed_quad,y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions_tree_reg = tree_reg.predict(X_test)\n",
    "test_predictions_best_model_svr = best_model_svr.predict(X_test)\n",
    "test_predictions_best_model_kneigh = best_model_kneigh.predict(X_test)\n",
    "test_predictions_lin_reg_intonly = lin_reg_intonly.predict(X_test_intonly)\n",
    "test_predictions_quad_reg = quad_reg.predict(X_test_quad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Decision Tree Regression'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_tree_reg)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_tree_reg))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_tree_reg)\n",
    "map_variable(y_test-test_predictions_tree_reg, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Support Vector Regression'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_best_model_svr)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_best_model_svr))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_best_model_svr)\n",
    "map_variable(y_test-test_predictions_best_model_svr, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for K Nearest Neighbors Regression'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_best_model_kneigh)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_best_model_kneigh))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_best_model_kneigh)\n",
    "map_variable(y_test-test_predictions_best_model_kneigh, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with Interactions'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_lin_reg_intonly)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test_intonly, test_predictions_lin_reg_intonly))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test_intonly, test_predictions_lin_reg_intonly)\n",
    "map_variable(y_test_intonly-test_predictions_lin_reg_intonly, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Quadratic Regression without Interactions'\n",
    "print 'Test R2: ',r2_score(y_test_quad, test_predictions_quad_reg)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test_quad, test_predictions_quad_reg))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test_quad, test_predictions_quad_reg)\n",
    "map_variable(y_test_quad-test_predictions_quad_reg, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.regressor import ResidualsPlot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tree_reg_pred_cv = cross_val_predict(tree_reg, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(tree_reg_pred_cv, tree_reg_pred_cv-y_train, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_tree_reg, test_predictions_tree_reg-y_test, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=0, xmax=.35, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Decision Tree Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reg_svr_best_model_pred_cv = cross_val_predict(best_model_svr, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(reg_svr_best_model_pred_cv, reg_svr_best_model_pred_cv-y_train, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_best_model_svr, test_predictions_best_model_svr-y_test, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=0, xmax=.35, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Tuned Support Vector Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "kneigh_best_model_pred_cv = cross_val_predict(best_model_kneigh, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(kneigh_best_model_pred_cv, kneigh_best_model_pred_cv-y_train, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_best_model_kneigh, test_predictions_best_model_kneigh-y_test, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=-.05, xmax=.35, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Tuned K Neighbor Regression')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Takes too long\n",
    "#lin_reg_intonly_pred_cv = cross_val_predict(lin_reg_intonly, X_train_intonly, y_train_intonly, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plt.scatter(lin_reg_intonly_pred_cv, lin_reg_intonly_pred_cv-y_train_intonly, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_lin_reg_intonly, test_predictions_lin_reg_intonly-y_test_intonly, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=y_train_intonly.min()-1, xmax=y_train_intonly.max()+1, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Linear Regression with Interactions')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Takes too long\n",
    "#quad_reg_pred_cv = cross_val_predict(quad_reg, X_train_quad, y_train_quad, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"plt.scatter(quad_reg_pred_cv, quad_reg_pred_cv-y_train_quad, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_quad_reg, test_predictions_quad_reg-y_test_quad, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='CV Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=y_train_intonly.min()-1, xmax=y_train_intonly.max()+1, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Quadratic Regression')\n",
    "plt.show()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best model After Adding Ratios and Doing Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lin_reg = linear_model.LinearRegression(fit_intercept=True, normalize=False)\n",
    "lin_reg.fit(X_train, y_train)\n",
    "test_predictions_ratios_lin_reg = lin_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Evaluation Metrics for Linear Regression with Ratio Features & Feature Selection'\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_ratios_lin_reg)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_ratios_lin_reg))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_ratios_lin_reg)\n",
    "map_variable(y_test-test_predictions_ratios_lin_reg, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ratios_lin_reg_pred_cv = cross_val_predict(lin_reg, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(ratios_lin_reg_pred_cv, ratios_lin_reg_pred_cv-y_train, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_ratios_lin_reg, test_predictions_ratios_lin_reg-y_test, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=-.3, xmax=.4, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Linear Regression with Ratio Features & Feature Selection')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's introduce some degree of regularization to see if we can decrease validation RMSE even further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lasso = linear_model.Lasso()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-8, -0.5, 30)\n",
    "param_grid = [{'alpha': alphas},]\n",
    "n_folds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(lasso, param_grid, cv=n_folds,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_lasso = np.sqrt(-gs.cv_results_['mean_train_score'])\n",
    "train_scores_std_lasso = np.sqrt(-gs.cv_results_['std_train_score'])\n",
    "val_scores_lasso = np.sqrt(-gs.cv_results_['mean_test_score'])\n",
    "val_scores_std_lasso = np.sqrt(-gs.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10, 6)\n",
    "plt.semilogx(alphas, val_scores_lasso)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = val_scores_std_lasso / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, val_scores_lasso + std_error, 'b--')\n",
    "plt.semilogx(alphas, val_scores_lasso - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, val_scores_lasso + std_error, val_scores_lasso - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(val_scores_lasso), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_lasso = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing Model Coefficients by using best model parameters obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for a in alphas:\n",
    "    lasso = linear_model.Lasso(alpha=a, fit_intercept=False)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    coefs.append(lasso.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Lasso Coefficients as a Function of Alpha')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions_lasso = best_model_lasso.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_lasso, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'Test R2: ',r2_score(y_test, test_predictions_lasso)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_lasso))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, np.abs(best_model_lasso.coef_)):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Lasso_Coef'})\n",
    "importances = importances.sort_values(by='Lasso_Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Number of Nonzero coefficients: \", len(importances[importances['Lasso_Coef']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model_lasso)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now trying Ridge:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge = linear_model.Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-8, -0.5, 30)\n",
    "param_grid = [{'alpha': alphas},]\n",
    "n_folds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(ridge, param_grid, cv=n_folds,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_ridge = np.sqrt(-gs.cv_results_['mean_train_score'])\n",
    "train_scores_std_ridge = np.sqrt(-gs.cv_results_['std_train_score'])\n",
    "val_scores_ridge = np.sqrt(-gs.cv_results_['mean_test_score'])\n",
    "val_scores_std_ridge = np.sqrt(-gs.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10, 6)\n",
    "plt.semilogx(alphas, val_scores_ridge)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = val_scores_std_ridge / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, val_scores_ridge + std_error, 'b--')\n",
    "plt.semilogx(alphas, val_scores_ridge - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, val_scores_ridge + std_error, val_scores_ridge - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(val_scores_ridge), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing Model Coefficients by using best model parameters obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.Ridge(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Ridge Coefficients as a Function of Alpha')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_ridge = gs.best_estimator_\n",
    "test_predictions_ridge = best_model_ridge.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_ridge)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_ridge))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_ridge, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, np.abs(best_model_ridge.coef_)):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'Ridge_Coef'})\n",
    "importances = importances.sort_values(by='Ridge_Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Number of Nonzero coefficients: \", len(importances[importances['Ridge_Coef']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model_ridge)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now Using ElasticNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "en = linear_model.ElasticNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alphas = np.logspace(-8, -0.5, 30)\n",
    "param_grid = [{'alpha': alphas},]\n",
    "n_folds = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(en, param_grid, cv=n_folds,scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_en = np.sqrt(-gs.cv_results_['mean_train_score'])\n",
    "train_scores_std_en = np.sqrt(-gs.cv_results_['std_train_score'])\n",
    "val_scores_en = np.sqrt(-gs.cv_results_['mean_test_score'])\n",
    "val_scores_std_en = np.sqrt(-gs.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure().set_size_inches(10, 6)\n",
    "plt.semilogx(alphas, val_scores_en)\n",
    "\n",
    "# plot error lines showing +/- std. errors of the scores\n",
    "std_error = val_scores_std_en / np.sqrt(n_folds)\n",
    "\n",
    "plt.semilogx(alphas, val_scores_en + std_error, 'b--')\n",
    "plt.semilogx(alphas, val_scores_en - std_error, 'b--')\n",
    "\n",
    "# alpha=0.2 controls the translucency of the fill color\n",
    "plt.fill_between(alphas, val_scores_en + std_error, val_scores_en - std_error, alpha=0.2)\n",
    "\n",
    "plt.ylabel('CV score +/- std error')\n",
    "plt.xlabel('alpha')\n",
    "plt.axhline(np.max(val_scores_en), linestyle='--', color='.5')\n",
    "plt.xlim([alphas[0], alphas[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Comparing Model Coefficients by using best model parameters obtained above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = linear_model.ElasticNet(alpha=a, fit_intercept=False)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = plt.gca()\n",
    "\n",
    "ax.plot(alphas, coefs)\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlim(ax.get_xlim()[::-1])  # reverse axis\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('weights')\n",
    "plt.title('Elastic Net Coefficients as a Function of Alpha')\n",
    "plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_en = gs.best_estimator_\n",
    "test_predictions_en = best_model_en.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_en)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_en))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_en, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feats = {} # a dict to hold feature_name: feature_importance\n",
    "for feature, importance in zip(X_train.columns, np.abs(best_model_en.coef_)):\n",
    "    feats[feature] = importance #add the name/value pair \n",
    "\n",
    "importances = pd.DataFrame.from_dict(feats, orient='index').rename(columns={0: 'EN_Coef'})\n",
    "importances = importances.sort_values(by='EN_Coef', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"Number of Nonzero coefficients: \" ,len(importances[importances['EN_Coef']>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model_en)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomForest Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Random Forest Regressor Does very well on both Training and Validation - a promising model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30], 'max_features':[8,10,12]},\n",
    "    {'bootstrap': [True,False], 'n_estimators':[3,10], 'max_features':[10,12]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "new_for_reg = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(new_for_reg, param_grid, cv=4, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfr_scores = cross_validate(gs, X_train, y_train, cv=6, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"CV Training RMSE: \", np.sqrt(-rfr_scores['train_score'].mean())\n",
    "print \"CV Validation RMSE: \",np.sqrt(-rfr_scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs_results_rf = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_model = gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp = rf_best_model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "imp = pd.DataFrame(sorted(zip(X_ratios.columns, imp),reverse=True,key=itemgetter(1)), columns=['Feature', 'Importance']).sort_values(by='Importance', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are strongly overfitting. Can this be improved with feature selection? Let's look at how many top features minimize validation error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_rmse_features(rf_best_model, X_train, y_train, imp.Feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We select the top n features such that n minimizes training error\n",
    "The above needs to be run again - there was an error in the viz method - for now I am picking the top 80 most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios[list(imp.Feature.head(50))], y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfr_best = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
    "           max_features=12, max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
    "           min_impurity_split=None, min_samples_leaf=1,\n",
    "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "           n_estimators=30, n_jobs=1, oob_score=False, random_state=None,\n",
    "           verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_rfr_scores = cross_validate(rfr_best, X_train, y_train, cv=6, scoring=('r2','neg_mean_squared_error'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"CV Training RMSE: \", best_rfr_scores['train_r2'].mean()\n",
    "print \"CV Validation RMSE: \",best_rfr_scores['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print \"CV Training RMSE: \", np.sqrt(-best_rfr_scores['train_neg_mean_squared_error'].mean())\n",
    "print \"CV Validation RMSE: \",np.sqrt(-best_rfr_scores['test_neg_mean_squared_error'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfr_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_predictions_rf = rfr_best.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_rf)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_rf))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_rf, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfr_best_train_pred_cv = cross_val_predict(rfr_best, X_train, y_train, cv=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(rfr_best_train_pred_cv, rfr_best_train_pred_cv-y_train, \n",
    "            c='steelblue', marker='o', edgecolor='white',\n",
    "           label='CV Train Data')\n",
    "plt.scatter(test_predictions_rf, test_predictions_rf-y_test, \n",
    "            c='limegreen', marker='x', edgecolor='red',\n",
    "           label='Test Data')\n",
    "plt.xlabel('Predicted Values')\n",
    "plt.ylabel('Residuals')\n",
    "plt.legend(loc='upper right')\n",
    "plt.hlines(y=0, color='black', xmin=-.1, xmax=.4, lw=3)\n",
    "plt.title('Predicted Values vs Residuals - Tuned Random Forest Regressor')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_ratios, y_normed, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "br = BaggingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30], 'max_features':[i for i in range(31,81,20)]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(br, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.best_estimator_\n",
    "gs_results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_br = np.sqrt(-gs.cv_results_['mean_train_score'])\n",
    "train_scores_std_br = np.sqrt(-gs.cv_results_['std_train_score'])\n",
    "val_scores_br = np.sqrt(-gs.cv_results_['mean_test_score'])\n",
    "val_scores_std_br = np.sqrt(-gs.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_br = gs.best_estimator_\n",
    "test_predictions_br = best_model_br.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_br)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_br))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_br)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_br, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model_br)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Adaboost "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "abr = AdaBoostRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(abr, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.best_estimator_\n",
    "gs_results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_abr = np.sqrt(-gs.cv_results_['mean_train_score'])\n",
    "train_scores_std_abr = np.sqrt(-gs.cv_results_['std_train_score'])\n",
    "val_scores_abr = np.sqrt(-gs.cv_results_['mean_test_score'])\n",
    "val_scores_std_abr = np.sqrt(-gs.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_abr = gs.best_estimator_\n",
    "test_predictions_abr = best_model_abr.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_abr)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_abr))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_abr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_abr, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model_abr)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#and Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gbr = GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "    {'n_estimators':[3,10,30]},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(gbr, param_grid, cv=3, scoring='neg_mean_squared_error')\n",
    "gs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print gs.best_params_\n",
    "print gs.best_estimator_\n",
    "gs_results = gs.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_scores_gbr = np.sqrt(-gs.cv_results_['mean_train_score'])\n",
    "train_scores_std_gbr = np.sqrt(-gs.cv_results_['std_train_score'])\n",
    "val_scores_gbr = np.sqrt(-gs.cv_results_['mean_test_score'])\n",
    "val_scores_std_gbr = np.sqrt(-gs.cv_results_['std_test_score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "best_model_gbr = gs.best_estimator_\n",
    "test_predictions_gbr = best_model_gbr.predict(X_test)\n",
    "print 'Test R2: ',r2_score(y_test, test_predictions_gbr)\n",
    "print 'Test RMSE: ',np.sqrt(mean_squared_error(y_test, test_predictions_gbr))\n",
    "print 'Test MAE: ',mean_absolute_error(y_test, test_predictions_gbr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "map_variable(y_test-test_predictions_gbr, listings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = ResidualsPlot(best_model_gbr)\n",
    "\n",
    "visualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\n",
    "visualizer.score(X_test, y_test)  # Evaluate the model on the test data\n",
    "g = visualizer.poof()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for First Batch of Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ['Linear', 'Decision Tree', 'Support Vector', 'KNN', 'Linear w Interactions', 'Quadratic', 'Linear w Feature Sel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_errors = [np.sqrt(-scores_lin['train_neg_mean_squared_error'].mean()),\n",
    "                np.sqrt(-scores_tree['train_neg_mean_squared_error'].mean())+.001,\n",
    "                np.sqrt(-gs_results_svreg['mean_train_score'].mean()),\n",
    "                np.sqrt(-gs_results_nn['mean_train_score'].mean()),\n",
    "                np.sqrt(-scores_lin_intonly['train_neg_mean_squared_error'].mean())+.001, \n",
    "                np.sqrt(-scores_quad['train_neg_mean_squared_error'].mean())+.001, \n",
    "                np.sqrt(-scores_lin_ratios['train_neg_mean_squared_error'].mean()),\n",
    "                np.sqrt(-scores_lin_ratios_fsel['train_neg_mean_squared_error'].mean())]\n",
    "\n",
    "val_errors = [np.sqrt(-scores_lin['test_neg_mean_squared_error'].mean()),\n",
    "              np.sqrt(-scores_tree['test_neg_mean_squared_error'].mean()),\n",
    "              np.sqrt(-gs_results_svreg['mean_test_score'].mean()),\n",
    "              np.sqrt(-gs_results_nn['mean_test_score'].mean()),\n",
    "              np.sqrt(-scores_lin_intonly['test_neg_mean_squared_error'].mean()),\n",
    "              np.sqrt(-scores_quad['test_neg_mean_squared_error'].mean()), \n",
    "              np.sqrt(-scores_lin_ratios['test_neg_mean_squared_error'].mean()),\n",
    "              np.sqrt(-scores_lin_ratios_fsel['test_neg_mean_squared_error'].mean())]\n",
    "\n",
    "test_errors = [lin_reg_rmse_test,\n",
    "            np.sqrt(mean_squared_error(y_test, test_predictions_tree_reg)),\n",
    "            np.sqrt(mean_squared_error(y_test, test_predictions_best_model_svr)),\n",
    "            np.sqrt(mean_squared_error(y_test, test_predictions_best_model_kneigh)),\n",
    "            np.sqrt(mean_squared_error(y_test_intonly, test_predictions_lin_reg_intonly)),\n",
    "            np.sqrt(mean_squared_error(y_test_quad, test_predictions_quad_reg)),\n",
    "               test_lin_ratios_nofs,\n",
    "            np.sqrt(mean_squared_error(y_test, test_predictions_ratios_lin_reg))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "x1 = [i-0.2 for i in range(len(train_errors))]\n",
    "x2 = [i for i in range(len(train_errors))]\n",
    "x3 = [i+0.2 for i in range(len(train_errors))]\n",
    "ax.bar(x1, train_errors, width=0.2, color='b', align='center')\n",
    "ax.bar(x2, val_errors, width=0.2, color='g', align='center')\n",
    "ax.bar(x3, test_errors, width=0.2, color='r', align='center')\n",
    "ax.set_xticklabels(X)\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator([i-0.05 for i in x2]))\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Regression Models Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for Regularized Regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sqrt(-scores_quad['train_neg_mean_squared_error'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ['Lasso', 'Ridge', 'Elastic Net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_errors = [train_scores_lasso.mean(), train_scores_ridge.mean(), train_scores_en.mean()]\n",
    "val_errors = [val_scores_lasso.mean(), val_scores_ridge.mean(), val_scores_en.mean()]\n",
    "test_errors = [np.sqrt(mean_squared_error(y_test, test_predictions_lasso)),\n",
    "               np.sqrt(mean_squared_error(y_test, test_predictions_ridge)), \n",
    "               np.sqrt(mean_squared_error(y_test, test_predictions_en))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sqrt(mean_squared_error(y_test, test_predictions_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ax = plt.subplot(111)\n",
    "x1 = [i-0.2 for i in range(len(train_errors))]\n",
    "x2 = [i for i in range(len(train_errors))]\n",
    "x3 = [i+0.2 for i in range(len(train_errors))]\n",
    "ax.bar(x1, train_errors, width=0.2, color='b', align='center')\n",
    "ax.bar(x2, val_errors, width=0.2, color='g', align='center')\n",
    "ax.bar(x3, test_errors, width=0.2, color='r', align='center')\n",
    "ax.set_xticklabels(X)\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator([i-0.05 for i in x2]))\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Regularized Models Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot for Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = ['Random Forest', 'Bagging', 'AdaBoost', 'Gradient Boosting']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.sqrt(-best_rfr_scores['train_neg_mean_squared_error'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_errors = [np.sqrt(-best_rfr_scores['train_neg_mean_squared_error'].mean()),\n",
    "                train_scores_br.mean(),\n",
    "                train_scores_abr.mean(),\n",
    "                train_scores_gbr.mean()]\n",
    "\n",
    "val_errors = [np.sqrt(-best_rfr_scores['test_neg_mean_squared_error'].mean()),\n",
    "              val_scores_br.mean(),\n",
    "              val_scores_abr.mean(),\n",
    "              val_scores_gbr.mean()]\n",
    "\n",
    "test_errors = [np.sqrt(mean_squared_error(y_test, test_predictions_rf)),\n",
    "               np.sqrt(mean_squared_error(y_test, test_predictions_br)),\n",
    "               np.sqrt(mean_squared_error(y_test, test_predictions_abr)),\n",
    "               np.sqrt(mean_squared_error(y_test, test_predictions_gbr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.ticker as ticker\n",
    "ax = plt.subplot(111)\n",
    "x1 = [i-0.2 for i in range(len(train_errors))]\n",
    "x2 = [i for i in range(len(train_errors))]\n",
    "x3 = [i+0.2 for i in range(len(train_errors))]\n",
    "ax.bar(x1, train_errors, width=0.2, color='b', align='center')\n",
    "ax.bar(x2, val_errors, width=0.2, color='g', align='center')\n",
    "ax.bar(x3, test_errors, width=0.2, color='r', align='center')\n",
    "ax.set_xticklabels(X)\n",
    "ax.xaxis.set_major_locator(ticker.FixedLocator([i-0.05 for i in x2]))\n",
    "ax.set_xlabel('Models')\n",
    "ax.set_ylabel('RMSE')\n",
    "ax.set_title('Ensembles Comparison')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA To explore the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_comps = 200\n",
    "pca = PCA(n_components=n_comps)\n",
    "model_fit = pca.fit(X_ratios)\n",
    "X_transform = model_fit.transform(X_ratios)\n",
    "print'Score for PCA with %i Components: %i' %(n_comps,model_fit.score(X_ratios))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight: 25 components will tell us everything! We will run Gridsearch CV to confirm later but looking at this plot would be enough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "print 'Explained Variance: '\n",
    "print pd.Series(model_fit.explained_variance_).head()\n",
    "print '\\n'\n",
    "print 'Explained Variance Ratio: '\n",
    "print pd.Series(model_fit.explained_variance_ratio_).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.features.pca import PCADecomposition\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "\n",
    "visualizer = PCADecomposition(n_components=25, scale=False, colormap='autumn')\n",
    "visualizer.fit(X_normed)\n",
    "visualizer.transform(X_normed)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.features.pca import PCADecomposition\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "\n",
    "visualizer = PCADecomposition(n_components=25, scale=False, colormap='autumn')\n",
    "visualizer.fit(X_ratios)\n",
    "visualizer.transform(X_ratios)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from yellowbrick.features.pca import PCADecomposition\n",
    "from yellowbrick.features.importances import FeatureImportances\n",
    "\n",
    "\n",
    "visualizer = PCADecomposition(n_components=25, scale=False, proj_dim=3, c='r')\n",
    "visualizer.fit(X_normed)\n",
    "visualizer.transform(X_normed)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "visualizer = PCADecomposition(scale=False,n_components=25, proj_dim=3, c='y')\n",
    "visualizer.fit(X_ratios)\n",
    "visualizer.transform(X_ratios)\n",
    "visualizer.poof()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_lr = linear_model.LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "comps = [i for i in range(20,28,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('pca', pca), ('reg', test_lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator = GridSearchCV(pipe, dict(pca__n_components=comps), cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.fit(X_normed, y_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight: 26 is actually the optimal number of components!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "estimator.best_estimator_.named_steps['reg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now actually running the regression on the reduced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_normed, y_normed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(n_components=26)\n",
    "lr = linear_model.LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps=[('pca', pca), ('reg', lr)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe.fit(X_train,y_train)\n",
    "pipe.predict(X_test)\n",
    "pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Insight: with this rudimental, untuned linear regression on 60 principal components we obtained an R2 of 58%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Just checking that it's the same as above...\n",
    "print 'R2 with PCA with 60 components: ', r2_score(y_test,pipe.predict(X_test))\n",
    "print 'RMSE with PCA with 60 components: ', mean_squared_error(y_test,pipe.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print 'CV Accuracy: ', cross_val_score(pipe, X_train, y_train, cv=10).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
